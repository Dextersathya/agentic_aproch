# -*- coding: utf-8 -*-
"""agentic_aproch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YqWVW3cUfBu-Qiz_lHDj10UfB_-35Kq6
"""

!pip install langgraph-checkpoint
!pip install langgraph-checkpoint-sqlite

!pip install langgraph-checkpoint-sqlite

pip install langchain_groq

pip install dotenv

pip install langgraph

from typing import Annotated

from typing_extensions import TypedDict

from langgraph.graph import StateGraph,START,END
from langgraph.graph.message import add_messages

!ollama run llama3

class State(TypedDict):
    # Messages have the type "list". The `add_messages` function
    # in the annotation defines how this state key should be updated
    # (in this case, it appends messages to the list, rather than overwriting them)
    messages:Annotated[list,add_messages]

import os
from dotenv import load_dotenv
load_dotenv()

from langchain_groq import ChatGroq
from langchain.chat_models import init_chat_model

llm=ChatGroq(model="llama3-8b-8192")

llm=init_chat_model("groq:llama3-8b-8192")
llm

def chatbot(state:State):
    return {"messages":[llm.invoke(state["messages"])]}

graph_builder=StateGraph(State)

## Adding node
graph_builder.add_node("llmchatbot",chatbot)
## Adding Edges
graph_builder.add_edge(START,"llmchatbot")
graph_builder.add_edge("llmchatbot",END)

## compile the graph
graph=graph_builder.compile()

## Visualize the graph
from IPython.display import Image,display

try:
    display(Image(graph.get_graph().draw_mermaid_png()))
except Exception:
    pass

response=graph.invoke({"messages":"Hi"})

response["messages"][-1].content

for event in graph.stream({"messages":"Hi how are you?"}):
    for value in event.values():
        print(value["messages"][-1].content)

!pip install langchain_tavily

from langchain_tavily import TavilySearch

tool=TavilySearch(max_results=2)
tool.invoke("what is langchain?")

def multiply(a:int,b:int)->int:
    """Multiply a and b

    Args:
        a (int): first int
        b (int): second int

    Returns:
        int: output int
    """
    return a*b

tools=[tool,multiply]

llm_with_tool=llm.bind_tools(tools)

llm_with_tool

from langgraph.graph import StateGraph,START,END
from langgraph.prebuilt import ToolNode
from langgraph.prebuilt import tools_condition

def tool_calling_llm(state:State):
    return {"messages":[llm_with_tool.invoke(state["messages"])]}

builder = StateGraph(State)
builder.add_node("tool_calling_chat", tool_calling_llm)
builder.add_node("tools", ToolNode(tools))

builder.add_edge(START, "tool_calling_chat")
builder.add_conditional_edges("tool_calling_chat", tools_condition)
builder.add_edge("tools", END)

graph = builder.compile()

display(Image(graph.get_graph().draw_mermaid_png()))

response=graph.invoke({"messages":"what is the recent ai news"})

response["messages"][-1].content

for m in response["messages"]:
  m.pretty_print()

response=graph.invoke({"messages":"what is 5 muliplied by 2"})
for m in response["messages"]:
  m.pretty_print()

"""# REACT agent

"""

from langgraph.graph import StateGraph,START,END
from langgraph.prebuilt import ToolNode
from langgraph.prebuilt import tools_condition
def tool_calling_llm(state:State):
    return {"messages":[llm_with_tool.invoke(state["messages"])]}

builder = StateGraph(State)
builder.add_node("tool_calling_chat", tool_calling_llm)
builder.add_node("tools", ToolNode(tools))

builder.add_edge(START, "tool_calling_chat")
builder.add_conditional_edges("tool_calling_chat", tools_condition)
builder.add_edge("tools","tool_calling_chat")
graph = builder.compile()

display(Image(graph.get_graph().draw_mermaid_png()))

response=graph.invoke({"messages":"give me the react ai news and what is 5 muliplied by 2"})
for m in response["messages"]:
  m.pretty_print()

"""# ADDING MEMORY IN AGENTIC GRAPH"""

# For in-memory checkpointing
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import StateGraph,START,END
from langgraph.prebuilt import ToolNode
from langgraph.prebuilt import tools_condition

memory=MemorySaver()

def tool_calling_llm(state:State):
    return {"messages":[llm_with_tool.invoke(state["messages"])]}

builder = StateGraph(State)
builder.add_node("tool_calling_chat", tool_calling_llm)
builder.add_node("tools", ToolNode(tools))

builder.add_edge(START, "tool_calling_chat")
builder.add_conditional_edges("tool_calling_chat", tools_condition)
builder.add_edge("tools","tool_calling_chat")
graph = builder.compile(checkpointer=memory)

display(Image(graph.get_graph().draw_mermaid_png()))

config={"configurable":{"thread_id":"1"}}
response=graph.invoke({"messages":"hi my name is dexter"},config=config)

response["messages"][-1].content

response=graph.invoke({"messages":"what is my name"},config=config)

response["messages"][-1].content

"""# STREAMING"""

from langgraph.checkpoint.memory import MemorySaver
memory=MemorySaver()

def superbot(state:State):
  return {"messages":[llm.invoke(state["messages"])]}

graph=StateGraph(State)
graph.add_node("superbot",superbot)
graph.add_edge(START,"superbot")
graph.add_edge("superbot",END)

graph_builder=graph.compile(checkpointer=memory)

from IPython.display import Image,display
display(Image(graph_builder.get_graph().draw_mermaid_png()))

config={"configurable":{"thread_id":"3"}}

for chunk in graph_builder.stream({"messages":"hi my name is dexter and i like cricket"},config,stream_mode="updates"):

  print(chunk)

for chunk in graph_builder.stream({"messages":"hi my name is dexter and i like cricket"},config,stream_mode="values"):

  print(chunk)

pip install langchain

import os
from langchain.chat_models import init_chat_model
llm=init_chat_model("groq:llama3-8b-8192")

from typing import Annotated
from langchain_tavily import TavilySearch
from langchain_core.tools import tool
from typing_extensions import TypedDict

from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import StateGraph,START,END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode
from langgraph.prebuilt import tools_condition

from langgraph.types import Command,interrupt

class State(TypedDict):
  messages: Annotated[list,add_messages]

graph_builder=StateGraph(State)

@tool
def human_assistance(query:str)-> str: